# 第五次作业文档说明

## 特别说明

> 本次作业文本检索部分使用了whoosh，使用原因已在上次报告中给出，其他部分和课程要求内容完全一致，环境使用anaconda3，具体环境在报告中给出。

## 文件说明  

>文件说明中，主要对于文件的用途，产生方式进行介绍。

在总文件夹下，有img文件夹，codes文件夹，readme.md，report.tex，report.pdf文件，img文件夹存放用于书写报告用的截图，图片等，readmemd，report.tex，report.pdf文件用于对于作业进行说明。

codes文件夹中包括：

>爬虫模块

+ source文件夹，用于存放爬虫爬下来的页面，这里由于所有的文件过于庞大，所以仅仅放了300个左右的页面以供展示。
+ Crawler.py : 加入了BloomFilter多线程爬虫的实现
+ index.txt : Crawler.py 中爬取的的页面在本地存储文件名以及对应的网页链接

>索引以及搜索模块

+ index_doc文件夹 : 用于存放网页索引的文件夹，文件夹中有更具体的存储文件，不再进一步作说明。
+ index_img文件夹 : 用于存放图片索引的文件夹，文件夹中有更具体的存储文件，不再进一步作说明
+ Index.py : 用于使用 whoosh 建立两个索引，包括网页和图片，写入字段的各种属性。
+ Write_doc.py : 用于写入网页文档索引，从souce文件夹中的源文件以及index.txt中提取信息，对于索引进行文档写入。
+ Write_img.py : 用于写入图片文档索引，从souce文件夹中的源文件以及index.txt中提取信息，对于索引进行文档写入。
+ Search_doc.py : 用于对于文档索引进行搜索，为方便于flask对接，将搜索写为一个函数，返回对象为一个包含所有匹配结果的一个列表。每个结果用字典结构呈现。
+ Search_doc.py : 用于对于图片进行搜索，为方便于flask对接，将搜索写为一个函数，返回对象为一个包含所有匹配结果的一个列表。每个结果用字典结构呈现。


>网页模块

+ templates文件夹 ：用于存放flask的网页模版
+ app.py : flask应用程序，用于实现网页

## 测试方法说明

>对于如何实现作业中的要求，的流程进行说明

### 爬取

对于爬取页面，总体来说沿用爬虫作业的代码，如果需要进行测试，请打开 Crawler.py 文件夹，修改 seed（种子）以及MAXPAGE（最大爬取页面）进行爬取，爬取结果会自动储存在source 文件夹中，以及 index.txt 文件中。

注意， source 中存储的是仅经过解码的原始网页，对于网页信息的处理留在后面进行。

### 建立索引以及写入

打开 Index.py 文件直接运行即可建立索引，需要修改字段具体值可以自行进行修改。

对于写入，打开 Write_doc.py 文件，或者Write_img_doc.py文件夹,直接运行，即可对于索引进行写入。

如果需要重新写入索引，请删去index文件夹并重复以上的步骤。

### 搜索

打开 Search_doc.py,或者Search_img.py文件夹，可以直接在主程序中对于搜索进行测试，改变主程序中的search函数参数为你所喜欢的关键词，并对于结果进行打印即可对于搜索进行测试。

### 网页

直接运行 app.py 文件并在本地的8080端口，打开浏览器进行测试，输入关键字进行搜索。