# 第三次作业文档说明

## 文件说明  

>文件说明中，主要对于文件的用途，产生方式进行介绍。

在总文件夹下，有img文件夹，codes文件夹，readme.md，report.tex，report.pdf文件，img文件夹存放用于书写报告用的截图，图片等，readmemd，report.tex，report.pdf文件用于对于作业进行说明。

codes文件夹中包括：

>爬虫模块

+ html文件夹，用于存放爬虫爬下来的页面，这里由于所有的文件过于庞大，所以仅仅放了50个左右的页面以供展示。

+ BloomFilter.py :实现BloomFilter的模块，用于实现crawler.py

+ crawler.py : 加入了BloomFilter多线程爬虫的实现

+ index.txt : crawler.py 中爬取的  *“用于展示”* 的页面在本地存储文件名以及对应的网络链接

+ index_15000.txt : crawler.py 中爬取的  *“所有的（总计15000个）”* 页面在本地存储文件名以及对应的网络链接

>索引以及搜索模块

+ index文件夹 : 用于存放 *“用于展示”* 的索引的文件夹，文件夹中有更具体的存储文件，不再进一步作说明。

+ index_15000文件夹 : 用于存放 *“所有的（总计15000个）”* 索引的文件夹，文件夹中有更具体的存储文件，不再进一步作说明。

+ IndexFiles.py : 用于针对html文件夹中的文件产生索引（即index文件夹）的实现

+ SearchFiles.py : 用于针对索引进行搜索的实现

## 测试方法说明

>对于如何实现作业中的要求，的流程进行说明

### 爬取

对于爬取页面，总体来说沿用上一次作业的代码，如果需要进行测试，请打开 crawler.py 文件夹，修改 seed（种子）以及MAXPAGE（最大爬取页面）进行爬取，爬取结果会自动储存在html 文件夹中，以及 index.txt 文件中。

注意， html 中存储的是仅经过解码的原始网页，对于网页信息的处理留在后面进行。

### 建立索引

打开 IndexFiles.py 文件，在主程序中，找到 IndexFiles() 函数，函数的三个参量分别代表：

+ 需要建立索引的文件们所在的根文件夹路径

+ 需要写入索引信息的文件夹路径

+ 存储文件以及文件来源（网络链接）的 txt 文件路径

如果需要重新建立索引，请按照上面提示填入不同的信息，再直接运行程序即可建立索引。

建立索引过程中，每添加100个文档就会打印已完成的文档数，作为进度控制，取代了adding语句，使得程序进度更加可视。如果文档中没有内容，则会打印文档中没有内容的语句在终端中。

### 完成搜索

打开 SearchFiles.py 文件夹，修改主程序中的 STORE_DIR 参数，改变查询的索引文件夹，修改run()函数的command参数，修改为需要查询的字符，由于爬取的是中文网站，希望你能输入中文关键字。

我们默认进行查找的用户是不会使用布尔查询语句的普通用户，所以对查询语句进行了分词并插入了AND逻辑连接词，使得查询结果更加有效，如果你希望使用布尔查询，应当注释掉对于查询语句进行处理的那个语句，再自己在命令中实现布尔查询，不然可能会出现未知错误。

搜索结果会在终端中打印，打印信息会包括运行时间，命中的文档数，命中的文档的各种信息等。

## 特别说明

>由于本次实验的特殊性——爬取的原始网页数据量太大，无法直接将原始网页提交，所以在这次的提交文件中，做了一些妥协。

+ html 文件夹仅仅展示爬取的50个页面

+ 提交在本地运行过后爬取所有页面的 index_15000.txt 文件

+ 提交基于 html 文件夹的 index.txt 文件

+ Indexfiles 分别针对展示的页面和全部页面建立索引，建立了 index 以及 index_15000

+ SearchFiles 默认针对 index_15000 进行搜索
